import faiss
import numpy as np
from transformers import pipeline
from sentence_transformers import SentenceTransformer

# 1Ô∏è‚É£ Load the embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# 2Ô∏è‚É£ Sample knowledge base
documents = [
    "RAG stands for Retrieval-Augmented Generation.",
    "RAG combines retrieval and generation to improve response accuracy.",
    "Large Language Models (LLMs) like GPT struggle with real-time updates.",
    "Retrievers fetch relevant documents from an external knowledge base.",
    "FAISS is a library for efficient similarity search."
]

# 3Ô∏è‚É£ Convert documents to embeddings
doc_embeddings = np.array(embedding_model.encode(documents), dtype=np.float32)

# 4Ô∏è‚É£ Build FAISS index
dimension = doc_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(doc_embeddings)

# 5Ô∏è‚É£ Define retrieval function
def retrieve_relevant_docs(query, top_k=2):
    query_embedding = np.array(embedding_model.encode([query]), dtype=np.float32)
    distances, indices = index.search(query_embedding, top_k)
    return [documents[i] for i in indices[0]]

# 6Ô∏è‚É£ Load a text generation model (FIXED)
generator = pipeline("text-generation", model="gpt2")

# 7Ô∏è‚É£ Define the RAG pipeline
def simple_rag_pipeline(query):
    retrieved_docs = retrieve_relevant_docs(query)
    context = " ".join(retrieved_docs)
    prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
    response = generator(prompt, max_length=255, num_return_sequences=1)
    return response[0]['generated_text']

# 8Ô∏è‚É£ Test the script
query = "What is RAG?"
response = simple_rag_pipeline(query)

print("\nüîç Query:", query)
print("üí° Retrieved Documents:", retrieve_relevant_docs(query))
print("üìù Generated Response:", response)
